ARG VERSION=v0.6.6.post1

FROM vllm/vllm-openai:${VERSION}

#ENV PIP_INDEX_URL=https://mirrors.sustech.edu.cn/pypi/web/simple

RUN pip install --no-cache-dir prometheus_fastapi_instrumentator

COPY ./vllm/entrypoints/openai/api_server.py /tmp/api_server.py

RUN mv /tmp/api_server.py $(pip show vllm | grep Location | awk '{print $2}')/vllm/entrypoints/openai/api_server.py

ENV ENABLE_METRICS=1

ENV ENABLE_LOG_REQUESTS=1

# add more dependencies

RUN apt-get update && apt-get install -y git && apt-get clean

#RUN pip install --no-cache-dir qwen-vl-utils

# remove flash-attn for now
# now tiktoken is a dependency of vllm, so we can remove it
#RUN pip install --no-cache-dir --no-build-isolation tiktoken